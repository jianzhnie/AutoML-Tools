{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "We will demonstrate how to use the LightGBM quantile regressor with TrainRegressor and ComputeModelStatistics on the Triazines dataset.\n",
    "\n",
    "This sample demonstrates how to use the following APIs:\n",
    "\n",
    "- TrainRegressor\n",
    "- LightGBMRegressor\n",
    "- ComputeModelStatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adult dataset we are going to use is publicly available at the UCI Machine Learning Repository. This data derives from census data, and consists of information about 48842 individuals and their annual income. We will use this information to predict if an individual earns <=50K or >50k a year. The dataset is rather clean, and consists of both numeric and categorical variables.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "- age: continuous\n",
    "- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "- fnlwgt: continuous\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n",
    "- education-num: continuous\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "- sex: Female, Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n",
    "- Target/Label: - <=50K, >50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.11:0.18.1\") \\\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "            .getOrCreate()\n",
    "import mmlspark\n",
    "from mmlspark.lightgbm import LightGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField(\"age\", DoubleType(), False),\n",
    "  StructField(\"workclass\", StringType(), False),\n",
    "  StructField(\"fnlwgt\", DoubleType(), False),\n",
    "  StructField(\"education\", StringType(), False),\n",
    "  StructField(\"education_num\", DoubleType(), False),\n",
    "  StructField(\"marital_status\", StringType(), False),\n",
    "  StructField(\"occupation\", StringType(), False),\n",
    "  StructField(\"relationship\", StringType(), False),\n",
    "  StructField(\"race\", StringType(), False),\n",
    "  StructField(\"sex\", StringType(), False),\n",
    "  StructField(\"capital_gain\", DoubleType(), False),\n",
    "  StructField(\"capital_loss\", DoubleType(), False),\n",
    "  StructField(\"hours_per_week\", DoubleType(), False),\n",
    "  StructField(\"native_country\", StringType(), False),\n",
    "  StructField(\"income\", StringType(), False)\n",
    "])\n",
    "\n",
    "dataset = spark.read.format(\"csv\").schema(schema).load(\"/home/robin/datatsets/adult/adult.data\")\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records read: 32561\n",
      "Schema: \n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age          workclass    fnlwgt   education  education_num  \\\n",
       "0  39.0          State-gov   77516.0   Bachelors           13.0   \n",
       "1  50.0   Self-emp-not-inc   83311.0   Bachelors           13.0   \n",
       "2  38.0            Private  215646.0     HS-grad            9.0   \n",
       "3  53.0            Private  234721.0        11th            7.0   \n",
       "4  28.0            Private  338409.0   Bachelors           13.0   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  income  \n",
       "0        2174.0           0.0            40.0   United-States   <=50K  \n",
       "1           0.0           0.0            13.0   United-States   <=50K  \n",
       "2           0.0           0.0            40.0   United-States   <=50K  \n",
       "3           0.0           0.0            40.0   United-States   <=50K  \n",
       "4           0.0           0.0            40.0            Cuba   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some basic info\n",
    "print(\"records read: \" + str(dataset.count()))\n",
    "print(\"Schema: \")\n",
    "dataset.printSchema()\n",
    "dataset.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Since we are going to try algorithms like Logistic Regression, we will have to convert the categorical variables in the dataset into numeric variables. There are 2 ways we can do this.\n",
    "\n",
    "- Category Indexing\n",
    "\n",
    "This is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}. This introduces an implicit ordering among your categories, and is more suitable for ordinal variables (eg: Poor: 0, Average: 1, Good: 2)\n",
    "\n",
    "- One-Hot Encoding\n",
    "\n",
    "This converts categories into binary vectors with at most one nonzero value (eg: (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0]))\n",
    "\n",
    "In this dataset, we have ordinal variables like education (Preschool - Doctorate), and also nominal variables like relationship (Wife, Husband, Own-child, etc). For simplicity's sake, we will use One-Hot Encoding to convert all categorical variables into binary vectors. It is possible here to improve prediction accuracy by converting each categorical column with an appropriate method.\n",
    "\n",
    "Here, we will use a combination of StringIndexer and OneHotEncoderEstimator to convert the categorical variables. The OneHotEncoderEstimator will return a SparseVector. Note: OneHotEncoderEstimator is renamed as OneHotEncoder in Spark 3.0.\n",
    "\n",
    "Since we will have more than 1 stage of feature transformations, we use a Pipeline to tie the stages together. This simplifies our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n",
    "        from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    else:\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code basically indexes each categorical column using the StringIndexer, and then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row.\n",
    "\n",
    "We use the StringIndexer again to encode our labels to label indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a VectorAssembler to combine all the feature columns into a single vector column. This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the stages as a Pipeline. This puts the data through all of the feature transformations we described in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- workclassIndex: double (nullable = false)\n",
      " |-- workclassclassVec: vector (nullable = true)\n",
      " |-- educationIndex: double (nullable = false)\n",
      " |-- educationclassVec: vector (nullable = true)\n",
      " |-- marital_statusIndex: double (nullable = false)\n",
      " |-- marital_statusclassVec: vector (nullable = true)\n",
      " |-- occupationIndex: double (nullable = false)\n",
      " |-- occupationclassVec: vector (nullable = true)\n",
      " |-- relationshipIndex: double (nullable = false)\n",
      " |-- relationshipclassVec: vector (nullable = true)\n",
      " |-- raceIndex: double (nullable = false)\n",
      " |-- raceclassVec: vector (nullable = true)\n",
      " |-- sexIndex: double (nullable = false)\n",
      " |-- sexclassVec: vector (nullable = true)\n",
      " |-- native_countryIndex: double (nullable = false)\n",
      " |-- native_countryclassVec: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>...</th>\n",
       "      <th>relationshipIndex</th>\n",
       "      <th>relationshipclassVec</th>\n",
       "      <th>raceIndex</th>\n",
       "      <th>raceclassVec</th>\n",
       "      <th>sexIndex</th>\n",
       "      <th>sexclassVec</th>\n",
       "      <th>native_countryIndex</th>\n",
       "      <th>native_countryclassVec</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age          workclass    fnlwgt   education  education_num  \\\n",
       "0  39.0          State-gov   77516.0   Bachelors           13.0   \n",
       "1  50.0   Self-emp-not-inc   83311.0   Bachelors           13.0   \n",
       "2  38.0            Private  215646.0     HS-grad            9.0   \n",
       "3  53.0            Private  234721.0        11th            7.0   \n",
       "4  28.0            Private  338409.0   Bachelors           13.0   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   ...  relationshipIndex       relationshipclassVec  raceIndex  \\\n",
       "0  ...                1.0  (0.0, 1.0, 0.0, 0.0, 0.0)        0.0   \n",
       "1  ...                0.0  (1.0, 0.0, 0.0, 0.0, 0.0)        0.0   \n",
       "2  ...                1.0  (0.0, 1.0, 0.0, 0.0, 0.0)        0.0   \n",
       "3  ...                0.0  (1.0, 0.0, 0.0, 0.0, 0.0)        1.0   \n",
       "4  ...                4.0  (0.0, 0.0, 0.0, 0.0, 1.0)        1.0   \n",
       "\n",
       "           raceclassVec sexIndex  sexclassVec native_countryIndex  \\\n",
       "0  (1.0, 0.0, 0.0, 0.0)      0.0        (1.0)                 0.0   \n",
       "1  (1.0, 0.0, 0.0, 0.0)      0.0        (1.0)                 0.0   \n",
       "2  (1.0, 0.0, 0.0, 0.0)      0.0        (1.0)                 0.0   \n",
       "3  (0.0, 1.0, 0.0, 0.0)      0.0        (1.0)                 0.0   \n",
       "4  (0.0, 1.0, 0.0, 0.0)      1.0        (0.0)                 9.0   \n",
       "\n",
       "                              native_countryclassVec label  \\\n",
       "0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.0   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.0   \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.0   \n",
       "3  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.0   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.0   \n",
       "\n",
       "                                            features  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "3  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(dataset)\n",
    "preppedDataDF = pipelineModel.transform(dataset)\n",
    "preppedDataDF.printSchema()\n",
    "preppedDataDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           features\n",
       "0    0.0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1    0.0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2    0.0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "3    0.0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4    0.0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"]\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "dataset.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22838\n",
      "9723\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.lightgbm import LightGBMClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lgb = LightGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boostingType='gbdt',\n",
    "    isUnbalance=True,\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    maxBin=60,\n",
    "    baggingFreq=1,\n",
    "    baggingSeed=696,\n",
    "    earlyStoppingRound=30,\n",
    "    learningRate=0.1,\n",
    "    lambdaL1=1.0,\n",
    "    lambdaL2=45.0,\n",
    "    maxDepth=3,\n",
    "    numLeaves=128,\n",
    "    baggingFraction=0.7,\n",
    "    featureFraction=0.7,\n",
    "    numIterations=800,\n",
    "    verbosity=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "装载各个阶段到Pipeline流水线中，执行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightGBMClassificationModel_263f97e5e776"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.transform(trainingData)\n",
    "test_preds = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看模型训练效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.9403789003910289\n",
      "Test AUC: 0.9228858550840612\n"
     ]
    }
   ],
   "source": [
    "binaryEvaluator = BinaryClassificationEvaluator()\n",
    "print (\"Train AUC: \" + str(binaryEvaluator.evaluate(train_preds, {binaryEvaluator.metricName: \"areaUnderROC\"})))\n",
    "print (\"Test AUC: \" + str(binaryEvaluator.evaluate(test_preds, {binaryEvaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们可以把预测概率结果和真实label取出来，方便进行计算其他自定义指标，例如KS等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob_list = [row.probability[0] for row in train_preds.select('probability').collect()]\n",
    "train_label_list = [row.label for row in train_preds.select('label').collect()]\n",
    " \n",
    "test_prob_list = [row.probability[0] for row in test_preds.select('probability').collect()]\n",
    "test_label_list = [row.label for row in test_preds.select('label').collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame在经过一系列transform之后，会多出4列，分别为features|rawPrediction|probability|prediction|，rawPrediction是树模型最后的一对儿得分，和为0；在经过sigmoid之后，得到probability里的一对儿概率值，其和为1，分别表示模型判定该样本为两个分类的可能性；而prediction则是模型预测的样本类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM调参之PySpark + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mmlspark\n",
    "from mmlspark.lightgbm import LightGBMClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lgb = LightGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boostingType='gbdt',\n",
    "    isUnbalance=True,\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    maxBin=60,\n",
    "    baggingFreq=1,\n",
    "    baggingSeed=696,\n",
    "    earlyStoppingRound=30,\n",
    "    learningRate=0.1,\n",
    "    # lambdaL1=1.0,\n",
    "    # lambdaL2=45.0,\n",
    "    maxDepth=3,\n",
    "    numLeaves=128,\n",
    "    baggingFraction=0.7,\n",
    "    featureFraction=0.7,\n",
    "    minSumHessianInLeaf=0.001,\n",
    "    numIterations=800,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置Grid Search参数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lgb.lambdaL1, list(np.arange(1.0, 3.0, 1.0))) \\\n",
    "    .addGrid(lgb.lambdaL2, list(np.arange(1.0, 4.0, 1.0))) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置完成之后，我们可以看一下参数都是哪些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([1.0, 1.0])\n",
      "dict_values([1.0, 2.0])\n",
      "dict_values([1.0, 3.0])\n",
      "dict_values([2.0, 1.0])\n",
      "dict_values([2.0, 2.0])\n",
      "dict_values([2.0, 3.0])\n"
     ]
    }
   ],
   "source": [
    "for param in paramGrid:\n",
    "    print(param.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证选择模型\n",
    "官方提供了两种模型选择的方式：CrossValidator和TrainValidationSplit，可以参考官方文档。CrossValidator和TrainValidationSplit的区别在于：CrossValidator会每次选取一部分训练集建模，去预测另外一部分训练集，这样会有K个预测的分数（K折交叉验证），最后模型的预测分数为K个分数的平均；而TrainValidationSplit则只会训练预测一次。这里，我们试着给出一个CrossValidator的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "cross_vallidator = CrossValidator(estimator=lgb,\n",
    "                          estimatorParamMaps=paramGrid, \n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3)\n",
    "model = cross_vallidator.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以得到最好的模型，并看最好的模型的特征重要性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBMClassificationModel_31fe08e1785d\n",
      "[66.0, 78.0, 44.0, 7.0, 28.0, 25.0, 24.0, 0.0, 46.0, 53.0, 32.0, 20.0, 18.0, 16.0, 16.0, 10.0, 19.0, 4.0, 11.0, 8.0, 6.0, 5.0, 2.0, 84.0, 49.0, 45.0, 12.0, 31.0, 9.0, 57.0, 39.0, 57.0, 35.0, 48.0, 49.0, 39.0, 20.0, 38.0, 25.0, 49.0, 24.0, 36.0, 9.0, 41.0, 55.0, 43.0, 50.0, 63.0, 47.0, 28.0, 23.0, 16.0, 89.0, 45.0, 22.0, 20.0, 14.0, 4.0, 5.0, 9.0, 0.0, 4.0, 10.0, 5.0, 0.0, 11.0, 6.0, 6.0, 0.0, 1.0, 0.0, 11.0, 0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 837.0, 933.0, 317.0, 318.0, 275.0, 568.0]\n"
     ]
    }
   ],
   "source": [
    "print(model.bestModel)\n",
    "print(model.bestModel.getFeatureImportances())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，官方api查看每个结果对应的超参数却是非常不友好，我们只好自己想办法，这里我们参考了这篇博客 LightGBM Hyper Parameters Tuning in Spark："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_extract(model):\n",
    "    \"\"\"\n",
    "    function extact hyperparameter information from a CrossValidatorModel\n",
    "    input: a CrossValidatorModel instance, model fit by CrossValidator in pyspark.ml.tuning\n",
    "    output: a dictionary with key(hyperparameters setting), value(evaluator's metrics, r2, auc,...)\n",
    "    \"\"\"\n",
    "    length = len(model.avgMetrics)\n",
    "    res = {}\n",
    "    for i in range(length):\n",
    "        s = \"\"\n",
    "        paraDict = model.extractParamMap()[model.estimatorParamMaps][i]\n",
    "        for j in paraDict.keys():\n",
    "            s += str(j).split(\"__\")[1] + \"  \"\n",
    "            s += str(paraDict[j]) + \"  \"\n",
    "        res[s.strip()] = model.avgMetrics[i]\n",
    "    return {k: v for k, v in sorted(res.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambdaL1  1.0  lambdaL2  2.0': 0.9236232416179453,\n",
       " 'lambdaL1  1.0  lambdaL2  1.0': 0.9236650282485419,\n",
       " 'lambdaL1  1.0  lambdaL2  3.0': 0.9236839543034484,\n",
       " 'lambdaL1  2.0  lambdaL2  3.0': 0.9237781089311508,\n",
       " 'lambdaL1  2.0  lambdaL2  1.0': 0.9238784689337032,\n",
       " 'lambdaL1  2.0  lambdaL2  2.0': 0.9240805941795885}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_extract(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方调参探讨\n",
    "我们通过CrossValidator可以获得最佳的模型，但是会有一个问题：这个最佳是拟合训练集的最佳，而不是我们给出的验证集的最佳；即使是TrainValidationSplit，我们也不能自定义验证集并传入，只能随机选择验证集。这样对于那些样本时间先后顺序不敏感的数据是影响不大的，比如图像等，但是对于交易类数据，我们希望可以根据时间先后顺序自定义训练集，验证集和测试集，并且根据验证集的效果来确定最佳参数和模型。因此，我们就需要换种方式达到目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9237224880382701\n"
     ]
    }
   ],
   "source": [
    "lightGBMs = list()\n",
    "for lambdaL1 in list(np.arange(1.0, 3.0, 1.0)):\n",
    "    for lambdaL2 in list(np.arange(1.0, 4.0, 1.0)):\n",
    "        lightGBMs.append(lgb.setLambdaL1(lambdaL1).setLambdaL2(lambdaL2))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "metrics = []\n",
    "models = []\n",
    " \n",
    "# 选择验证集效果最好的模型\n",
    "for learner in lightGBMs:\n",
    "    model = learner.fit(trainingData)\n",
    "    models.append(model)\n",
    "    scoredData = model.transform(testData)\n",
    "    metrics.append(evaluator.evaluate(scoredData))\n",
    "best_metric = max(metrics)\n",
    "best_model = models[metrics.index(max(metrics))] \n",
    " \n",
    "# 得到测试集上AUC\n",
    "scored_test = best_model.transform(testData)\n",
    "print(evaluator.evaluate(scored_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM调参之PySpark + mmlspark + Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面, 我们记录了分别用PySpark中自带的CrossValidator和更通用的生成多个分类器同时执行训练预测的方式选取最好的模型。其中CrossValidator并不能得到验证集上最佳的分类器，而是得到训练集上最佳的效果。而mmlspark当中却有更为简单的方式，既可以得到验证集上最佳的效果，也可以方便地记录我们每一组参数对应的结果，是一种很好的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.lightgbm import LightGBMClassifier\n",
    "from mmlspark.automl import *\n",
    "from mmlspark.train import TrainClassifier, ComputeModelStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LightGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boostingType='gbdt',\n",
    "    isUnbalance=True,\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    maxBin=60,\n",
    "    baggingFreq=1,\n",
    "    baggingSeed=696,\n",
    "    earlyStoppingRound=20,\n",
    "    learningRate=0.1,\n",
    "    #lambdaL1=1.0,\n",
    "    #lambdaL2=45.0,\n",
    "    maxDepth=3,\n",
    "    numLeaves=128,\n",
    "    baggingFraction=0.7,\n",
    "    featureFraction=0.7,\n",
    "    minSumHessianInLeaf=0.001,\n",
    "    numIterations=800,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以lambdaL1和lambdaL2为例，设置4组不同的参数，这里可以自己记录每组参数，方便和后面各模型效果对应，就不再赘述了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightGBMs = list()\n",
    "for lambdaL1 in list(np.arange(1.0, 3.0, 1.0)):\n",
    "    for lambdaL2 in list(np.arange(1.0, 3.0, 1.0)):\n",
    "        lightGBMs.append(lgb.setLambdaL1(lambdaL1).setLambdaL2(lambdaL2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，这里可以用这种方式来设置参数，更方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lightGBMs = list()\n",
    "params = itertools.product([1.0, 2.0], [1.0, 2.0])\n",
    "for param in params:\n",
    "    lightGBMs.append(lgb.setLambdaL1(param[0]).setLambdaL2(param[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用mmlspark.train模块当中的TrainClassifier类训练模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_models = [TrainClassifier(model=lgb, labelCol=\"label\").fit(trainingData) for lgb in lightGBMs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用mmlspark.automl当中的FindBestModel类，寻找在验证集上效果最好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = FindBestModel(evaluationMetric='AUC', models=lgb_models).fit(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看看最好的模型效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(evaluation_type='Classification', confusion_matrix=DenseMatrix(2, 2, [6041.0, 353.0, 1307.0, 2022.0], False), accuracy=0.8292708011930474, precision=0.607389606488435, recall=0.8513684210526316, AUC=0.9234257227172484)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.getBestModelMetrics().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以看看所有4个模型在验证集上的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model_name='TrainClassifier_84a31e2a4d4d', metric=0.9234257227172484, parameters='featuresCol: TrainClassifier_229f52ed8c8e_features, labelCol: label, predictionCol: prediction, probabilityCol: probability, rawPredictionCol: rawPrediction'),\n",
       " Row(model_name='TrainClassifier_c8bc9ba50c7b', metric=0.9234257227172484, parameters='featuresCol: TrainClassifier_5f6badd6c4a9_features, labelCol: label, predictionCol: prediction, probabilityCol: probability, rawPredictionCol: rawPrediction'),\n",
       " Row(model_name='TrainClassifier_53de8573a8b8', metric=0.9234257227172484, parameters='featuresCol: TrainClassifier_2ca4d026570c_features, labelCol: label, predictionCol: prediction, probabilityCol: probability, rawPredictionCol: rawPrediction'),\n",
       " Row(model_name='TrainClassifier_e15e3f26dca1', metric=0.9234257227172484, parameters='featuresCol: TrainClassifier_2cc6adae0df3_features, labelCol: label, predictionCol: prediction, probabilityCol: probability, rawPredictionCol: rawPrediction')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.getAllModelMetrics().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's AUC on test set = 92.34%\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(testData)\n",
    "metrics = ComputeModelStatistics().transform(predictions)\n",
    "print(\"Best model's AUC on test set = \"\n",
    "      + \"{0:.2f}%\".format(metrics.first()[\"AUC\"] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看看metrics里面都是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(evaluation_type='Classification', confusion_matrix=DenseMatrix(2, 2, [6041.0, 353.0, 1307.0, 2022.0], False), accuracy=0.8292708011930474, precision=0.607389606488435, recall=0.8513684210526316, AUC=0.9234257227172484)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果也是一个list，包含混淆矩阵，accuracy,precision和recall："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总体来说，这种方式来进行分布式训练，比PySpark自带的api更方便一些，推荐mmlspark方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37864bitmlconda8c4f5daf70e14f63bbcfb2c70c66718a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
